{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from testCases_v4a import *\n",
    "#from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    Z = np.dot(W, A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward_test_case():\n",
    "    \"\"\"\n",
    "    X = np.array([[-1.02387576, 1.12397796],\n",
    "   [-1.62328545, 0.64667545],\n",
    "   [-1.74314104, -0.59664964]])\n",
    "    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "    b = 5\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    A_prev = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "    return A_prev, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    #print(\"L= \", L)\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        #print(\"l= \", l)\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    #print(\"l out= \", l)\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(l+1)], parameters[\"b\"+str(l+1)], \"sigmoid\") #use A instaed of A_prev as it has been updated. Use (l+1) instead of (l) as loop ended on (L-1) and we need the value at L\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    #print(AL.shape, \" \" , (1,X.shape[1]))\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward_test_case_2hidden():\n",
    "    np.random.seed(6)\n",
    "    X = np.random.randn(5,4)\n",
    "    W1 = np.random.randn(4,5)\n",
    "    b1 = np.random.randn(4,1)\n",
    "    W2 = np.random.randn(3,4)\n",
    "    b2 = np.random.randn(3,1)\n",
    "    W3 = np.random.randn(1,3)\n",
    "    b3 = np.random.randn(1,1)\n",
    "  \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return X, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    cost = (-1/m)*(np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)) # dont screw up the brackets\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case():\n",
    "    Y = np.asarray([[1, 1, 0]])\n",
    "    aL = np.array([[.8,.9,0.4]])\n",
    "    \n",
    "    return Y, aL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache): #where did we calculate dZ? ... go to the next function for it ðŸ˜Š\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    dW = (1/m)*(np.dot(dZ, A_prev.T))\n",
    "    db = (1/m)*(np.sum(dZ, axis=1, keepdims=True))\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_test_case():\n",
    "    \"\"\"\n",
    "    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],\n",
    "       [-1.62328545,  0.64667545],\n",
    "       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    dZ = np.random.randn(3,4)\n",
    "    A = np.random.randn(5,4)\n",
    "    W = np.random.randn(3,5)\n",
    "    b = np.random.randn(3,1)\n",
    "    linear_cache = (A, W, b)\n",
    "    return dZ, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward_test_case():\n",
    "    \"\"\"\n",
    "    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    dA = np.random.randn(1,2)\n",
    "    A = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "    Z = np.random.randn(1,2)\n",
    "    linear_cache = (A, W, b)\n",
    "    activation_cache = Z\n",
    "    linear_activation_cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return dA, linear_activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL <------------WHY?\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)): #<------------how does reversed work? why from L-2?\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward_test_case():\n",
    "    \"\"\"\n",
    "    X = np.random.rand(3,2)\n",
    "    Y = np.array([[1, 1]])\n",
    "    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}\n",
    "\n",
    "    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],\n",
    "           [ 0.02738759,  0.67046751],\n",
    "           [ 0.4173048 ,  0.55868983]]),\n",
    "    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),\n",
    "    np.array([[ 0.]])),\n",
    "   np.array([[ 0.41791293,  1.91720367]]))])\n",
    "   \"\"\"\n",
    "    np.random.seed(3)\n",
    "    AL = np.random.randn(1, 2)\n",
    "    Y = np.array([[1, 0]])\n",
    "\n",
    "    A1 = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    Z1 = np.random.randn(3,2)\n",
    "    linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "    A2 = np.random.randn(3,2)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "    Z2 = np.random.randn(1,2)\n",
    "    linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "    caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "    return AL, Y, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads(grads):\n",
    "    print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "    print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "    print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*(grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*(grads[\"db\" + str(l+1)])\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_test_case():\n",
    "    \"\"\"\n",
    "    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],\n",
    "        [-1.8634927 , -0.2773882 , -0.35475898],\n",
    "        [-0.08274148, -0.62700068, -0.04381817],\n",
    "        [-0.47721803, -1.31386475,  0.88462238]]),\n",
    " 'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],\n",
    "        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],\n",
    "        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),\n",
    " 'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],\n",
    "        [-0.16051336, -0.76883635, -0.23003072]]),\n",
    " 'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    " 'b2': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    " 'b3': np.array([[ 0.],\n",
    "        [ 0.]])}\n",
    "    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],\n",
    "        [ 0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ]]),\n",
    " 'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n",
    " 'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),\n",
    " 'da1': np.array([[ 0.70760786,  0.65063504],\n",
    "        [ 0.17268975,  0.15878569],\n",
    "        [ 0.03817582,  0.03510211]]),\n",
    " 'da2': np.array([[ 0.39561478,  0.36376198],\n",
    "        [ 0.7674101 ,  0.70562233],\n",
    "        [ 0.0224596 ,  0.02065127],\n",
    "        [-0.18165561, -0.16702967]]),\n",
    " 'da3': np.array([[ 0.44888991,  0.41274769],\n",
    "        [ 0.31261975,  0.28744927],\n",
    "        [-0.27414557, -0.25207283]]),\n",
    " 'db1': 0.75937676204411464,\n",
    " 'db2': 0.86163759922811056,\n",
    " 'db3': -0.84161956022334572}\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    np.random.seed(3)\n",
    "    dW1 = np.random.randn(3,4)\n",
    "    db1 = np.random.randn(3,1)\n",
    "    dW2 = np.random.randn(1,3)\n",
    "    db2 = np.random.randn(1,1)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STARTING ACTUAL EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d9efeb1f48>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('Pclass', 'Survived', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d9effd7b48>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEGCAYAAAADs9wSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATvUlEQVR4nO3df5BdZ33f8ffHMo4HYyDgbc1YMlJBQBQwuF6L0qTElB+R04yVBkhknAmeutEwRXYnBFxTqEpNKKnohEIjUgR1IUxAGNMkS0atmoBhEoOp1sHYyEbJVjZoJdSsMT9MkrGR/e0f98q5XV/pXK330f7Q+zWzs/c557nnftf36uPznHvOc1JVSJKO7bSFLkCSFjuDUpI6GJSS1MGglKQOBqUkdTh9oQs4Ueecc06tXr16ocuQtMzcdttt91XV2LB1Sy4oV69ezeTk5EKXIWmZSfKNY61z6C1JHQxKSerQNCiTbEiyL8lUkuuGrD8/yc1JvpLkjiQ/07IeSZqLZkGZZAWwHbgUWAdcnmTdrG5vB26sqguBTcAHWtUjSXPVco9yPTBVVfur6iFgJ7BxVp8Cntx//BTgUMN6JGlOWgblecCBgfZ0f9mgdwC/lGQa2AVcPWxDSTYnmUwyOTMz06JWSTqmlkGZIctmT1V0OfCRqloJ/AzwsSSPqamqdlTVeFWNj40NPc1JkpppGZTTwKqB9koeO7S+CrgRoKq+BJwJnNOwJkk6YS1PON8DrE2yBjhI78ua183q803g5cBHkvwYvaB0bC01cu2113L48GHOPfdctm3bttDlLBnNgrKqjiTZAuwGVgA3VNXeJNcDk1U1Afwa8KEkv0pvWH5lOZOw1Mzhw4c5ePDgQpex5DS9hLGqdtH7kmZw2daBx3cBP9GyBkl6vLwyR5I6GJSS1MGglKQOBqUkdTAoJamDQSlJHQxKSepgUEpSB4NSkjosuZuLSSfDN69/wUKX0MSR+58GnM6R+7+xbP/G87feOe/bdI9SkjoYlJLUwaCUpA4GpSR1MCglqYNBKUkdDEpJ6tA0KJNsSLIvyVSS64asf2+S2/s/f57kuy3rkaS5aHbCeZIVwHbglfTuyLgnyUT/9g8AVNWvDvS/GriwVT2SNFct9yjXA1NVtb+qHgJ2AhuP0/9y4BMN65GkOWkZlOcBBwba0/1lj5HkmcAa4HMN65GkOWkZlBmy7Fi3ot0E3FRVDw/dULI5yWSSyZkZb/st6eRqGZTTwKqB9krg0DH6buI4w+6q2lFV41U1PjY2No8lSlK3lkG5B1ibZE2SM+iF4cTsTkmeC/wo8KWGtUjSnDX71ruqjiTZAuwGVgA3VNXeJNcDk1V1NDQvB3ZW1bGG5ZLmyTlnPgIc6f/WqJrOR1lVu4Bds5ZtndV+R8saJP2tN1/gqcpz4ZU5ktTBoJSkDgalJHUwKCWpg0EpSR0MSknqYFBKUgeDUpI6GJSS1MGglKQOBqUkdTAoJamDQSlJHQxKSepgUEpSB4NSkjoYlJLUwaCUpA5NgzLJhiT7kkwlue4YfX4hyV1J9ib5eMt6JGkumt0zJ8kKYDvwSnq3rt2TZKKq7hrosxZ4K/ATVfWdJH+nVT2SNFct9yjXA1NVtb+qHgJ2Ahtn9fkVYHtVfQegqv6yYT2SNCctg/I84MBAe7q/bNBzgOckuSXJrUk2DNtQks1JJpNMzszMNCpXkoZrGZQZsmz2vbtPB9YCl9C7v/eHkzz1MU+q2lFV41U1PjY2Nu+FStLxtAzKaWDVQHslcGhInz+oqh9W1T3APnrBKUmLRsug3AOsTbImyRnAJmBiVp/fB14GkOQcekPx/Q1rkqQT1iwoq+oIsAXYDdwN3FhVe5Ncn+SyfrfdwLeT3AXcDLylqr7dqiZJmotmpwcBVNUuYNesZVsHHhfwpv6PJC1KXpkjSR0MSknqYFBKUgeDUpI6GJSS1MGglKQOBqUkdTAoJamDQSlJHQxKSepgUEpSB4NSkjoYlJLUwaCUpA4GpSR1MCglqYNBKUkdmgZlkg1J9iWZSnLdkPVXJplJcnv/55+3rEeS5qLZrSCSrAC2A6+kd7fFPUkmququWV0/WVVbWtUhSY9Xyz3K9cBUVe2vqoeAncDGhq8nSU20DMrzgAMD7en+stleneSOJDclWTVkPUk2J5lMMjkzM9OiVkk6ppZBmSHLalb7M8DqqroA+GPgo8M2VFU7qmq8qsbHxsbmuUxJOr6WQTkNDO4hrgQODXaoqm9X1YP95oeAixrWI0lz0jIo9wBrk6xJcgawCZgY7JDkGQPNy4C7G9YjSXPS7FvvqjqSZAuwG1gB3FBVe5NcD0xW1QRwTZLLgCPA/cCVreqRpLk6blAmeYDHHld8VFU9+XjPr6pdwK5Zy7YOPH4r8NaRKpWkBXLcoKyqswH6e4GHgY/R+5LmCuDs5tVJ0iIw6jHKn66qD1TVA1X1/ar6beDVLQuTpMVi1KB8OMkVSVYkOS3JFcDDLQuTpMVi1KB8HfALwP/t/7y2v0ySlr2RvvWuqnvx8kNJp6iR9iiTPCfJZ5N8rd++IMnb25YmSYvDqEPvD9E7jeeHAFV1B70TyCVp2Rs1KJ9YVf971rIj812MJC1GowblfUmeRf/k8ySvAb7VrCpJWkRGvYTxjcAO4HlJDgL30DvpXJKWvVGD8htV9YokZwGnVdUDLYuSpMVk1KH3PUl2AP8A+EHDeiRp0Rk1KJ9Lb2LdN9ILzd9K8pPtypKkxWOkoKyqv6mqG6vq54ELgScDX2hamSQtEiNP3Jvkp5J8APgz4Ex6lzRK0rI30pc5Se4BbgduBN5SVX/VtCpJWkRG/db7hVX1/aaVSNIi1TXD+bVVtQ14V5LHzHReVdd0PH8D8D56t4L4cFX9xjH6vQb4FHBxVU2OWrwknQxde5RHb/Z1wuGVZAWwHXglvTsy7kkyUVV3zep3NnAN8OUTfQ1JOhm6bgXxmf7DO6rqKye47fXAVFXtB0iyk95UbXfN6vdOYBvw5hPcviSdFKN+6/2bSb6e5J1JfnzE55wHHBhoT/eXPSrJhcCqqvrD420oyeYkk0kmZ2ZmRnx5SZofo55H+TLgEmAG2JHkzhHmo8ywTT26MjkNeC/wayO8/o6qGq+q8bGxsVFKlqR5M/J5lFV1uKreD7yB3qlCWzueMg2sGmivBA4NtM8Gng98Psm99C6PnEgyPmpNknQyjDrD+Y8leUd/hvPfAr5IL/iOZw+wNsmaJGfQm+h34ujKqvpeVZ1TVaurajVwK3CZ33pLWmxGPY/yvwGfAF5VVYe6OgNU1ZEkW4Dd9E4PuqGq9vbvET5ZVRPH34IkLQ6dQdk/zef/VNX7TnTjVbUL2DVr2dAhe1VdcqLbl6SToXPoXVUPA0/vD58l6ZQz8sS9wC1JJoBHr/Ouqt9sUpUkLSKjBuWh/s9p9L6tlqRTxkhBWVX/rnUhkrRYjTrN2s0MnCx+VFX943mvSJIWmVGH3oPXYZ8JvBrv6y3pFDHq0Pu2WYtuSeKtICSdEkYdej9toHkaMA6c26QiSVpkRh1638bfHqM8AtwLXNWiIElabLpmOL8YOFBVa/rt19M7Pnkvj51XUpKWpa4rcz4IPASQ5KXAu4GPAt8DdrQtTZIWh66h94qqur//+BeBHVX1aeDTSW5vW5okLQ5de5QrkhwN05cDnxtYN+rxTUla0rrC7hPAF5LcB/wN8CcASZ5Nb/gtScte183F3pXks8AzgP9VVUe/+T4NuLp1cZK0GHQOn6vq1iHL/rxNOZK0+Ix8zxxJOlUZlJLUoWlQJtmQZF+SqSTXDVn/hv6tb29P8qdJ1rWsR5LmollQ9u+1sx24FFgHXD4kCD9eVS+oqhcB2wBnTJe06LTco1wPTFXV/qp6CNgJbBzsUFXfH2iexZA5LyVpobU8afw84MBAexp48exOSd4IvAk4Axg6EXCSzcBmgPPPP3/eC5Wk42m5R5khy4bNkr69qp4F/Cvg7cM2VFU7qmq8qsbHxsbmuUxJOr6WQTkNrBpor6R3g7Jj2Qn8XMN6JGlOWgblHmBtkjX9e4JvAiYGOyRZO9D8J8BfNKxHkuak2THKqjqSZAuwG1gB3FBVe5NcD0xW1QSwJckrgB8C3wFe36oeSZqrpjMAVdUuYNesZVsHHv/Llq8vSfPBK3MkqYNBKUkdDEpJ6mBQSlIHg1KSOhiUktTBoJSkDgalJHUwKCWpg0EpSR0MSknqYFBKUgeDUpI6GJSS1KHpNGs6Oa699loOHz7Mueeey7Zt2xa6HGnZMSiXgcOHD3Pw4MGFLkNathx6S1KHpkGZZEOSfUmmklw3ZP2bktyV5I4kn03yzJb1SNJcNAvKJCuA7cClwDrg8iTrZnX7CjBeVRcANwEeYJO06LTco1wPTFXV/qp6iN7taDcOdqiqm6vqr/vNW+nd0laSFpWWQXkecGCgPd1fdixXAf9j2Iokm5NMJpmcmZmZxxIlqVvLoMyQZTW0Y/JLwDjwnmHrq2pHVY1X1fjY2Ng8lihJ3VqeHjQNrBporwQOze7Uv6/324CfqqoHG9YjSXPSMij3AGuTrAEOApuA1w12SHIh8EFgQ1X9ZcNauOgtv9Ny8wvq7PseYAXwzfseWLZ/523v+eWFLkGnsGZD76o6AmwBdgN3AzdW1d4k1ye5rN/tPcCTgE8luT3JRKt6JGmuml6ZU1W7gF2zlm0dePyKlq8vSfPBK3MkqYNBKUkdDEpJ6mBQSlIHg1KSOhiUktTBiXuXgUfOOOv/+y1pfhmUy8BfrX3VQpcgLWsOvSWpg0EpSR0MSknqYFBKUgeDUpI6GJSS1MGglKQOBqUkdTAoJalD06BMsiHJviRTSa4bsv6lSf4syZEkr2lZiyTNVbOgTLIC2A5cCqwDLk+ybla3bwJXAh9vVYckPV4tr/VeD0xV1X6AJDuBjcBdRztU1b39dY80rEOSHpeWQ+/zgAMD7en+shOWZHOSySSTMzMz81KcJI2qZVBmyLKay4aqakdVjVfV+NjY2OMsS5JOTMugnAZWDbRXAocavp4kNdEyKPcAa5OsSXIGsAmYaPh6ktREs6CsqiPAFmA3cDdwY1XtTXJ9kssAklycZBp4LfDBJHtb1SNJc9V0hvOq2gXsmrVs68DjPfSG5JK0aHlljiR1MCglqYNBKUkdDEpJ6mBQSlIHg1KSOhiUktTBoJSkDgalJHUwKCWpg0EpSR0MSknqYFBKUgeDUpI6GJSS1MGglKQOBqUkdTAoJalD06BMsiHJviRTSa4bsv5Hknyyv/7LSVa3rEeS5qJZUCZZAWwHLgXWAZcnWTer21XAd6rq2cB7gf/Qqh5JmquWe5Trgamq2l9VDwE7gY2z+mwEPtp/fBPw8iRpWJMknbCWd2E8Dzgw0J4GXnysPlV1JMn3gKcD9w12SrIZ2Nxv/iDJviYVL23nMOu/23KS//j6hS5hOVnWnxX+7Zz3tZ55rBUtg3JYtTWHPlTVDmDHfBS1XCWZrKrxha5Di5+flRPXcug9DawaaK8EDh2rT5LTgacA9zesSZJOWMug3AOsTbImyRnAJmBiVp8J4OiY6jXA56rqMXuUkrSQmg29+8cctwC7gRXADVW1N8n1wGRVTQD/FfhYkil6e5KbWtVzCvDQhEblZ+UExR04STo+r8yRpA4GpSR1MCiXoSSXJPnDha5DbSS5JsndSX630fbfkeTNLba9VLU8j1JSG/8CuLSq7lnoQk4V7lEuUklWJ/l6kg8n+VqS303yiiS3JPmLJOv7P19M8pX+7+cO2c5ZSW5Isqffb/ZlpFpCkvwX4O8BE0neNuy9TXJlkt9P8pkk9yTZkuRN/T63Jnlav9+v9J/71SSfTvLEIa/3rCT/M8ltSf4kyfNO7l+8OBiUi9uzgfcBFwDPA14H/CTwZuBfA18HXlpVFwJbgX8/ZBtvo3d+6sXAy4D3JDnrJNSuBqrqDfQu3HgZcBbHfm+fT+/zsh54F/DX/c/Jl4Bf7vf571V1cVW9ELib3iQ1s+0Arq6qi+h97j7Q5i9b3Bx6L273VNWdAEn2Ap+tqkpyJ7Ca3pVMH02ylt6ln08Yso1XAZcNHHM6Ezif3j8MLW3Hem8Bbq6qB4AH+nMofKa//E56/+MFeH6SXweeCjyJ3jnPj0ryJOAfAp8amKvmR1r8IYudQbm4PTjw+JGB9iP03rt30vsH8U/7c3l+fsg2Ary6qpxIZPkZ+t4meTHdnx2AjwA/V1VfTXIlcMms7Z8GfLeqXjS/ZS89Dr2XtqcAB/uPrzxGn93A1Uenr0ty4UmoSyfH431vzwa+leQJwBWzV1bV94F7kry2v/0keeHjrHlJMiiXtm3Au5PcQu8y0WHeSW9IfkeSr/XbWh4e73v7b4AvA39E73j3MFcAVyX5KrCXx84pe0rwEkZJ6uAepSR1MCglqYNBKUkdDEpJ6mBQSlIHg1JLWv96571J7khye/9ka2leeWWOlqwkLwF+Fvj7VfVgknOAMxa4LC1D7lFqKXsGcF9VPQhQVfdV1aEkFyX5Qn/Gm91JnpHk9P5MOZcAJHl3knctZPFaOjzhXEtWf9KGPwWeCPwx8Engi8AXgI1VNZPkF4Gfrqp/luTHgZuAa+hd1fTiqnpoYarXUuLQW0tWVf0gyUXAP6I3zdgngV+nN8XYH/UvgV4BfKvff2+Sj9GbSeclhqRGZVBqSauqh+nNmvT5/vRzbwT2VtVLjvGUFwDfBf7uyalQy4HHKLVkJXlufy7Oo15Eb57Nsf4XPSR5Qn/ITZKfB54OvBR4f5KnnuyatTR5jFJLVn/Y/Z/pTTx7BJgCNgMrgffTm4budOA/Ab9H7/jly6vqQJJrgIuq6vULUbuWFoNSkjo49JakDgalJHUwKCWpg0EpSR0MSknqYFBKUgeDUpI6/D/Xu1wCPQXswAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('Sex', 'Survived', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Pclass'].isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
